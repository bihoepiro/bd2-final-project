# -*- coding: utf-8 -*-
"""Spimi-prepro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MY08p1RkM_5fJnp8og9KNjg8wn-3pL4M
"""

import pandas as pd
import os
import nltk
nltk.download('punkt')
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer('english')

csv_file = '/content/data.csv'
df = pd.read_csv(csv_file)
#DICCIONARIO DOCS
diccionario_docs = {}
# Crear una carpeta para almacenar los archivos de texto
output_folder = '/content/text_files'
os.makedirs(output_folder, exist_ok=True)

stoplist = ["a", "an", "and", "are", "as", "at", "be", "but", "by",
            "for", "if", "in", "into", "is", "it", "no", "not", "of",
            "on", "or", "such", "that", "the", "their", "then", "there",
            "these", "they", "this", "to", "was", "will", "with", "&", ",", "."]

for index, row in df.iterrows():
    # Concatenar todos los atributos en una sola cadena
    row_text = ' '.join(row.astype(str).tolist())
    text_file_path = os.path.join(output_folder, f'fila_{index + 1}.txt')

    with open(text_file_path, 'w') as text_file:
        text_file.write(row_text)
   #TOKENIZAR (DIVIDIR EL TEXTO EN PALABRAS)
    with open( f'/content/text_files/fila_{index + 1}.txt', encoding = "utf-8") as textfile:
       words = textfile.read()
    words = nltk.word_tokenize(words.lower())
    words_ = []
    for word in words:
       if  word not in stoplist:
        words_.append(word)
    words_ = [stemmer.stem(words_[i]) for i in range(len(words_))]
    #como doc id le asignamos su index + 1
    diccionario_docs[index] = words_






print(f'Archivos de texto creados en la carpeta: {output_folder}')

print(diccionario_docs)

# TOKENIZAR (DIVIDIR EL TEXO EN PALABRAS)
