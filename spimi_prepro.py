# -*- coding: utf-8 -*-
"""Spimi-prepro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MY08p1RkM_5fJnp8og9KNjg8wn-3pL4M
"""

import pandas as pd
import os
import nltk
nltk.download('punkt')
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer('english')

def determinar_bloque(i, limite):
  for j in range(limite + 1):
    # seleccionar bloque
    if i <= j*limite and i > (j-1)*limite:
      return j

csv_file = '/content/data.csv'
df = pd.read_csv(csv_file)
#DICCIONARIO DOCS
diccionario_docs = {}



def obtener_tf(index, word):
  frecuencia = 0
  words = diccionario_docs[index]
  for w in words:
    if w == word:
      frecuencia += 1
  return frecuencia


# Crear una carpeta para almacenar los archivos de texto
output_folder = '/content/text_files'
os.makedirs(output_folder, exist_ok=True)

stoplist = ["a", "an", "and", "are", "as", "at", "be", "but", "by",
            "for", "if", "in", "into", "is", "it", "no", "not", "of",
            "on", "or", "such", "that", "the", "their", "then", "there",
            "these", "they", "this", "to", "was", "will", "with"]
diccionario_palabras = {}
for index, row in df.iterrows():
    # Concatenar todos los atributos en una sola cadena
    row_text = ' '.join(row.astype(str).tolist())
    text_file_path = os.path.join(output_folder, f'fila_{index + 1}.txt')

    with open(text_file_path, 'w') as text_file:
        text_file.write(row_text)
   #TOKENIZAR (DIVIDIR EL TEXTO EN PALABRAS)
    with open( f'/content/text_files/fila_{index + 1}.txt', encoding = "utf-8") as textfile:
       words = textfile.read()
    words = nltk.word_tokenize(words.lower())
    words_ = []
    for word in words:
       if  word not in stoplist:
        words_.append(word)
        #para garantizar el diccionario correcto
        #diccionario_palabras.setdefault(word, {})[index+1] = obtener_tf(index +1, word)
    words_ = [stemmer.stem(words_[i]) for i in range(len(words_))]
    #como doc id le asignamos su index + 1
    diccionario_docs[index+1] = words_
    for word in words_:
      diccionario_palabras.setdefault(word, {})[index+1] = obtener_tf(index +1, word)



print(f'Archivos de texto creados en la carpeta: {output_folder}')

print(diccionario_palabras)

print(diccionario_docs)

cant_bloques = 3
cant_docs = 30
bloques = {}
limite = cant_docs/cant_bloques


for i in range(cant_bloques):
  bloques[i] = {}

bloque_temp = 1
for i in range(cant_docs):
  #obtener datos necesarios es decir TF de cada word en cada doc, bueno en los necesarios por bloque
  bloque_temp = determinar_bloque(i, limite)
